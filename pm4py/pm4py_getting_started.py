# -*- coding: utf-8 -*-
"""pm4py-getting-started.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17sxo0fdpI5HJtxA-Vm_6AH8bhg5Pf77P

# Getting started to pm4py 

API documentation: https://pm4py.fit.fraunhofer.de/static/assets/api/2.5.0/api.html

This tutorial is based on: https://pm4py.fit.fraunhofer.de/static/assets/api/2.5.0/getting_started.html

This tutorial is divided into 4 parts:
1. Install **pm4py**
2. **Import**, **explore** and **preprocess** an XES event log
3. **Discover** a process model
4. **Performance analysis**

Let's start ðŸš€

## Install pm4py
"""

pip install pm4py

"""If you are working locally, you'll need to install graphviz, the library used by pm4py to visualize process models.

1.   Install Windows package from: https://graphviz.org/download/
2.   Install python graphviz package using: `pip install graphviz`
3. Add `C:\Program Files\Graphviz2.38\bin` to `User path`
4. Add `C:\Program Files\Graphviz2.38\bin\dot.exe` to `System path`

## Import & Preprocess Event log

Event logs can be either in [XES](https://xes-standard.org/) or CSV format. In order for the event log to be properly read and analyzed by pm4py, it must contain at least the `case id`, `activity name`, and `timestamp` attributes. These attributes are used to identify the unique cases, the specific activities that were performed, and the order in which they were performed.

Let's import the XES event log `running-example.xes` using `pm4py.read_xes() `.

First, **upload the .xes file to your notebook environment**.

Then you will need to `import pm4py`.
"""

import pm4py

log = pm4py.read_xes('running-example.xes')
log

"""The `log` object is a [pandas](https://pandas.pydata.org/) dataframe. Pandas is a popular python library for data manipulation and analysis. It provides powerful data structures, including the DataFrame and Series, which allow for easy handling and manipulation of large amounts of data. It also offers a wide range of tools for data cleaning, filtering, and transformation, as well as powerful data visualization capabilities through integration with libraries such as Matplotlib and Seaborn. 

Let's check what's inside the log ðŸ‘€

Each row in the dataframe corresponds to an event. An event has multiple attributes. The mandatory ones are `concept:name` which is the activity name, `time:timestamp` which is the time at which the activity is executed and `case:concept:name` which is the case identifier.

Let's explore the log by answering the following questions ðŸ¤”:
1. How the events are distributed over time?
2. What is the trace of the case id 3?
3. How many traces are there in the log? how many events?
4. How many distinct activities are there in the log? List them
5. What activities occur first, and what activities occur last in the traces?
6. What are the variants (identical traces) and their frequencies?

**1. How the events are distributed over time?**
"""

pm4py.view_events_per_time_graph(log, format='png')

"""**2. What is the trace of the case id 3?**"""

trace = log[log['case:concept:name']=='3']

trace

"""**3. How many traces are there in the log? how many events?**"""

nb_events = len(log) #length of the dataframe
nb_events

nb_traces = len(log['case:concept:name'].unique()) #length of the unique values of the case:concept:name column
nb_traces

"""**4. How many distinct activities are there in the log? List them**"""

unique_activities = log['concept:name'].unique()
print("There are {} unique activities".format(len(unique_activities)))
print(unique_activities)

"""**5. What activities occur first, and what activities occur last in the traces?**"""

start_activities = pm4py.get_start_activities(log)
end_activities = pm4py.get_end_activities(log)

print("start activities: ", start_activities)
print("end activities: ", end_activities)

"""**6. What are the variants (identical traces) and their frequencies?**"""

variants = pm4py.get_variants(log)
variants

#traces that end with pay compensate

"""**Let's now do some preprocessing to our log before we proceed.**

### Built-in filtering in pm4py

Let's say that we're only interested in:
1. Analyzing the traces that end with the activity "reject request".
2. Analyzing the activities performed by "Pete" and "Mike"
3. Analyzing the traces that were executed in 2011

**1. Filter in the traces that end with the activity "reject request"**
"""

filtered_log = pm4py.filter_end_activities(log, ['reject request'])
filtered_variants = pm4py.get_variants(filtered_log)
filtered_variants

"""**2. Keep only the activities that are performed by "Pete" and "Mike"**"""

filtered_log = pm4py.filter_event_attribute_values(log, 'org:resource', {'Pete', 'Mike'}, level='event')
filtered_log

"""**3. Filter in the traces that were executed in 2011**"""

import datetime as dt

filtered_log = pm4py.filter_time_range(log, dt.datetime(2011, 1, 1), dt.datetime(2011, 12, 31),
                                       mode='traces_contained')
filtered_log

"""## Discover a BPMN model

We will automatically discover a BPMN model from our log. There exists a plenty of [disocvery algorithms](https://www.processmining.org/process-discovery.html). In this tutorial we will use an algoritm called **inductive-miner** which allows us to discover a BPMN model.
"""

bpmn_model = pm4py.discover_bpmn_inductive(log)
pm4py.view_bpmn(bpmn_model)

"""### Performance analysis

Visualize a less formal model called **Directly-follows graph (DFG)** or **process map**
"""

dfg, start_activities, end_activities = pm4py.discover_directly_follows_graph(log)
pm4py.view_dfg(dfg, start_activities, end_activities)

"""Analyze the performance of the traces and visualize the result on the DFG"""

dfg, start_activity, end_activity = pm4py.discover_performance_dfg(log)
pm4py.view_performance_dfg(dfg, start_activity, end_activity)